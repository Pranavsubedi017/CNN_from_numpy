{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "xjxPxis2yVf_",
        "outputId": "c6b0335d-49fd-4b60-ebf5-68181b0af568"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ninput_array=np.random.randn(2000,50,50,3)\\nprint(input_array.shape)\\nfilter_size=(3,3)\\nlayer4=Convolve(input_array,filter_size,30,2,1)\\noutput_array=layer4.forward()\\nprint(output_array.shape)\\nlayer5=Max_Pool(output_array)\\noutput_array2=layer5.forward()\\nprint(output_array2.shape)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "\n",
        "class Convolve():\n",
        "  def __init__(self,activation=None,pooling=None,filter_size=(3,3),num_filters=1,stride=1,padding=0):\n",
        "\n",
        "    self.filter_size=filter_size\n",
        "    self.filter_height,self.filter_width=self.filter_size\n",
        "    self.num_filters=num_filters\n",
        "    self.stride=stride\n",
        "    self.padding=padding\n",
        "    self.pool=pooling\n",
        "    self.activation=activation\n",
        "    self.t = 0\n",
        "\n",
        "    # self.sdw = np.zeros((self.filter_height, self.filter_width, self.channel, self.num_filters))\n",
        "\n",
        "\n",
        "    # self.vdw =  np.zeros((self.filter_height,self.filter_width,self.channel,self.num_filters))\n",
        "  def get_patches(self,input_array,backward=False):\n",
        "    if backward==True:\n",
        "      self.padding2=self.padding\n",
        "    else:\n",
        "      self.padding2=0\n",
        "\n",
        "    self.input_array=input_array\n",
        "    # self.input_array=input_array\n",
        "    self.batch_size,self.height,self.width,self.channel=self.input_array.shape\n",
        "    self.output_height = int((self.height +2*self.padding2 - self.filter_height)/self.stride) + 1\n",
        "    self.output_width = int((self.width +2*self.padding2  - self.filter_width)/self.stride) + 1\n",
        "    self.new_shape = (self.batch_size, self.output_height, self.output_width, self.filter_height, self.filter_width, self.channel)\n",
        "    self.new_strides = (self.input_array.strides[0], self.stride * self.input_array.strides[1], self.stride * self.input_array.strides[2],\n",
        "                  self.input_array.strides[1], self.input_array.strides[2], self.input_array.strides[3])\n",
        "    self.patches = as_strided(self.input_array, self.new_shape, self.new_strides)\n",
        "    #print(patches.shape)\n",
        "    return self.patches\n",
        "\n",
        "  def forward(self,input_array):\n",
        "    self.input_array=input_array\n",
        "    #print(f'input array ko shape{self.input_array.shape}')\n",
        "    if self.padding > 0:\n",
        "      self.input_array_padded = np.pad(self.input_array, ((0, 0), (self.padding, self.padding), (self.padding, self.padding), (0, 0)), mode='constant')\n",
        "    else:\n",
        "      self.input_array_padded=self.input_array\n",
        "    self.patches=self.get_patches(self.input_array_padded)\n",
        "\n",
        "    #print(f'patches ko shape{self.patches.shape}')\n",
        "    self.patches=self.patches.reshape(self.patches.shape[0],self.patches.shape[1],self.patches.shape[2],-1)\n",
        "    #print(f'patches{self.patches[0][0][0]}')\n",
        "    #print(self.patches.shape)\n",
        "    self.filter = np.random.randn(self.filter_height,self.filter_width,self.channel,self.num_filters)\n",
        "    self.filter=self.filter.reshape(-1,self.num_filters)\n",
        "    #print(self.filter.shape)\n",
        "    self.output_array=np.tensordot(self.patches,self.filter,axes=([3],[0]))\n",
        "    self.patches=self.patches.reshape(self.batch_size,self.output_height,self.output_width,-1)\n",
        "    #print(f'output array{self.output_array.shape}')\n",
        "    self.output_array=self.output_array.reshape(self.batch_size,self.output_height,self.output_width,self.num_filters)\n",
        "    #print(f'output array{self.output_array.shape}')\n",
        "    if self.pool:\n",
        "      self.output_array=self.pool.forward(self.output_array)\n",
        "    if self.activation:\n",
        "          self.output_array=self.activation.forward(self.output_array)\n",
        "    #print(f'convolve forward{self.output_array.shape}')\n",
        "\n",
        "    return self.output_array\n",
        "\n",
        "  def l2(self):\n",
        "        return np.sum(self.filter ** 2)\n",
        "\n",
        "  def backward(self, gradient):\n",
        "    self.gradient=gradient\n",
        "\n",
        "    if self.activation:\n",
        "      self.gradient=self.activation.backward(self.gradient)\n",
        "\n",
        "    if self.pool:\n",
        "      self.gradient=self.pool.backward(self.gradient)\n",
        "    #print(f'convolve backward ma pako{self.gradient.shape}')\n",
        "    # print(f'patches shape RESHAPED{self.patches.reshape(self.batch_size, self.output_height, self.output_width, self.filter_height, self.filter_width, self.channel).shape}')\n",
        "    # self.patches=self.patches.reshape(self.batch_size, self.output_height, self.output_width, self.filter_height, self.filter_width, self.channel)\n",
        "    # self.gradient=self.gradient.reshape(self.batch_size,self.output_height,self.output_width,self.channel,-1)\n",
        "    #print(f'convolve backward{self.gradient.shape}')\n",
        "\n",
        "    self.filter_grad = np.tensordot(self.patches.transpose(3,0, 1, 2), self.gradient, axes=([1,2,3], [0, 1, 2]))\n",
        "    #print(f'filter grad{self.filter_grad.shape}')\n",
        "    # Reshape the filter gradient to match the original filter dimensions\n",
        "    self.filter_grad = self.filter_grad.reshape(self.filter_height, self.filter_width,self.channel, self.num_filters)\n",
        "    #print(f'filter grad reshape{self.filter_grad.shape}')\n",
        "    # Calculate gradient for the input\n",
        "    #gradient_input = np.zeros_like(self.input_array)\n",
        "    self.flipped_filter = np.flip(self.filter, axis=(0, 1))\n",
        "    self.flipped_filter=self.flipped_filter.reshape(-1,self.channel)\n",
        "    #print(f'flipped filter{self.flipped_filter.shape}')\n",
        "    self.gradient_patches = self.get_patches(self.gradient,backward=True)\n",
        "    #print(f'gradient patches{self.gradient_patches.shape}')\n",
        "    self.gradient_patches=self.gradient_patches.reshape(self.gradient_patches.shape[0],self.gradient_patches.shape[1],self.gradient_patches.shape[2],-1)\n",
        "    #print(f'gradient patches222222222{self.gradient_patches.shape}')\n",
        "    self.gradient_input = np.tensordot(self.gradient_patches, self.flipped_filter, axes=([3], [0]))\n",
        "    #print(f'gradient input{self.gradient_input.shape}')\n",
        "    # return self.gradient_input\n",
        "    # for i in range(self.output_height):\n",
        "    #     for j in range(self.output_width):\n",
        "    #         patch_gradient = np.tensordot(gradient[:, i, j, :], self.filter.T, axes=(1, 0))\n",
        "    #         patch_gradient = patch_gradient.reshape(self.batch_size, self.filter_height, self.filter_width, self.channel)\n",
        "    #         gradient_input[:, i*self.stride:i*self.stride+self.filter_height, j*self.stride:j*self.stride+self.filter_width, :] += patch_gradient\n",
        "\n",
        "    # if self.padding > 0:\n",
        "    #     print(f'gradient input padding{self.gradient_input.shape}')\n",
        "    #     self.gradient_input = self.gradient_input[:, self.padding:-self.padding, self.padding:-self.padding, :]\n",
        "    #     print(f'gradient input padding{self.gradient_input.shape}')\n",
        "\n",
        "    # else:\n",
        "    #     self.gradient_input=self.gradient_input\n",
        "\n",
        "    return self.gradient_input\n",
        "\n",
        "\n",
        "\n",
        "  def calculate(self, optimizer):\n",
        "\n",
        "\n",
        "    self.sdw = np.zeros_like(self.filter_grad)\n",
        "\n",
        "\n",
        "\n",
        "    self.vdw =  np.zeros_like(self.filter_grad)\n",
        "\n",
        "\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "        self.t += 1\n",
        "        beta1, beta2 = 0.9, 0.999\n",
        "        epsilon = 1e-8\n",
        "\n",
        "        sdw = beta2 * self.sdw + (1 - beta2) * (self.filter_grad ** 2)\n",
        "        self.sdw = sdw\n",
        "\n",
        "        vdw = beta1 * self.vdw + (1 - beta1) * self.filter_grad\n",
        "        self.vdw = vdw\n",
        "\n",
        "        # Bias correction for adam optimizer for the starting difference while using exponantially weighted average\n",
        "        sdw_corrected = self.sdw / (1 - beta2 ** self.t)\n",
        "\n",
        "        vdw_corrected = self.vdw / (1 - beta1 ** self.t)\n",
        "\n",
        "\n",
        "        self.sdw_corrected = sdw_corrected\n",
        "\n",
        "        self.vdw_corrected = vdw_corrected\n",
        "\n",
        "\n",
        "  def update(self, learning_rate, optimizer):\n",
        "    if optimizer == 'adam':\n",
        "        self.filter_grad -= learning_rate * self.vdw_corrected / (np.sqrt(self.sdw_corrected) + 1e-8)\n",
        "\n",
        "    else:\n",
        "        self.filter -= learning_rate * self.filter_grad\n",
        "\n",
        "\n",
        "\n",
        "class MaxPool():\n",
        "  def __init__(self,pool_size=(2,2),stride=2):\n",
        "\n",
        "    self.pool_size=pool_size\n",
        "    self.pool_height,self.pool_width=self.pool_size\n",
        "    self.stride=stride\n",
        "\n",
        "\n",
        "  def get_patches(self,input_array):\n",
        "    self.input_array=input_array\n",
        "    self.batch_size,self.height,self.width,self.channel=self.input_array.shape\n",
        "    self.output_height = int((self.height - self.pool_height)/self.stride) + 1\n",
        "    self.output_width = int((self.width - self.pool_width)/self.stride) + 1\n",
        "    self.new_shape = (self.batch_size, self.output_height, self.output_width, self.pool_height, self.pool_width, self.channel)\n",
        "    self.new_strides = (self.input_array.strides[0], self.stride * self.input_array.strides[1], self.stride * self.input_array.strides[2],\n",
        "    self.input_array.strides[1], self.input_array.strides[2], self.input_array.strides[3])\n",
        "    self.patches = as_strided(self.input_array, self.new_shape, self.new_strides)\n",
        "\n",
        "    return self.patches\n",
        "\n",
        "  def forward(self,input_array):\n",
        "    self.input_array=input_array\n",
        "\n",
        "    self.patches2=self.get_patches(self.input_array)\n",
        "    #print(f'patches2{self.patches2.shape}')\n",
        "    self.patches2=self.patches2.reshape(self.patches.shape[0],self.patches.shape[1],self.patches.shape[2],self.pool_height*self.pool_width,-1)\n",
        "    #print(f'patches2{self.patches2.shape}')\n",
        "    # print(f'patches{self.patches[0][0][0]}')\n",
        "    #print(f'patches ko shape{self.patches2.shape}')\n",
        "\n",
        "\n",
        "    self.output=np.max(self.patches2,axis=3)\n",
        "    #print(f'output of pool{self.output.shape}')\n",
        "    # self.output=self.output.reshape(self.batch_size,self.output_height,self.output_width,self.channel)\n",
        "    self.max_indices=np.argmax(self.patches2,axis=3)\n",
        "    #print(f'maxpool forward{self.output.shape}')\n",
        "\n",
        "    return self.output\n",
        "\n",
        "  # def l2(self):\n",
        "  #       return np.sum(self.output ** 2)\n",
        "\n",
        "  def backward(self, gradient):\n",
        "          maxpool_gradient=np.zeros_like(self.input_array)\n",
        "          self.maxpool_gradient=maxpool_gradient\n",
        "          gradient=gradient.flatten()\n",
        "          max_indices=self.max_indices.reshape(-1)\n",
        "\n",
        "\n",
        "          batch_indices,height_indices,width_indices,channel_indices= np.indices((self.batch_size,self.output_height,self.output_width,self.channel))\n",
        "          indexes= ( batch_indices.flatten(),\n",
        "                   (height_indices.flatten()*self.stride).reshape(-1) + max_indices //self.pool_width,\n",
        "                   (width_indices.flatten()*self.stride).reshape(-1)+max_indices % self.pool_width,\n",
        "\n",
        "                   channel_indices.flatten() )\n",
        "          self.maxpool_gradient[indexes]+=gradient.flatten()\n",
        "\n",
        "\n",
        "          self.maxpool_gradient=self.maxpool_gradient.reshape(self.batch_size,self.height,self.width,self.channel)\n",
        "          #print(f'maxpool backward{self.maxpool_gradient.shape}')\n",
        "          return self.maxpool_gradient\n",
        "\n",
        "class Flatten():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def forward(self,input_array):\n",
        "    self.input_array=input_array\n",
        "    self.batch_size,self.height,self.width,self.channel=self.input_array.shape\n",
        "    self.new_shape=(self.batch_size,self.height*self.width*self.channel)\n",
        "    self.output_array=self.input_array.reshape(self.new_shape)\n",
        "    #print(f'flatten forward{self.output_array.shape}')\n",
        "    return self.output_array\n",
        "\n",
        "  def l2(self):\n",
        "        return 1\n",
        "\n",
        "  def backward(self, gradient):\n",
        "        self.gradient=gradient\n",
        "        #print(f'flatten backward{self.gradient.reshape(self.input_array.shape).shape}')\n",
        "        return self.gradient.reshape(self.input_array.shape)\n",
        "\n",
        "\n",
        "  def calculate(self, optimizer):\n",
        "    pass\n",
        "\n",
        "  def update(self, learning_rate, optimizer):\n",
        "   pass\n",
        "'''\n",
        "input_array=np.random.randn(2000,50,50,3)\n",
        "print(input_array.shape)\n",
        "filter_size=(3,3)\n",
        "layer4=Convolve(input_array,filter_size,30,2,1)\n",
        "output_array=layer4.forward()\n",
        "print(output_array.shape)\n",
        "layer5=Max_Pool(output_array)\n",
        "output_array2=layer5.forward()\n",
        "print(output_array2.shape)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense():\n",
        "    def __init__(self, ninputs, nnodes,activation=None ):\n",
        "\n",
        "        self.weight = np.random.randn(ninputs, nnodes) * np.sqrt(2. / ninputs) #xaiver initialization\n",
        "        self.bias = np.random.rand(nnodes) * 0.01\n",
        "        self.sdw = np.zeros((ninputs, nnodes))\n",
        "        self.sdb = np.zeros(nnodes)\n",
        "        self.vdw = np.zeros((ninputs, nnodes))\n",
        "        self.vdb = np.zeros(nnodes)\n",
        "        self.t = 0\n",
        "        self.activation=activation\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        self.output = np.dot(inputs, self.weight) + self.bias\n",
        "        if self.activation:\n",
        "          self.output=self.activation.forward(self.output)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        if self.activation:\n",
        "          gradient=self.activation.backward(gradient)\n",
        "        self.gradient_weight = np.dot(self.input.T, gradient)\n",
        "        self.gradient_bias = np.sum(gradient, axis=0)\n",
        "        self.gradient_input = np.dot(gradient, self.weight.T)\n",
        "\n",
        "\n",
        "        return self.gradient_input\n",
        "\n",
        "    def calculate(self, optimizer):\n",
        "        if optimizer == 'adam':\n",
        "            self.t += 1\n",
        "            beta1, beta2 = 0.9, 0.999\n",
        "            epsilon = 1e-8\n",
        "\n",
        "            self.sdw = beta2 * self.sdw + (1 - beta2) * (self.gradient_weight ** 2)\n",
        "            self.sdb = beta2 * self.sdb + (1 - beta2) * (self.gradient_bias ** 2)\n",
        "\n",
        "            self.vdw = beta1 * self.vdw + (1 - beta1) * self.gradient_weight\n",
        "            self.vdb = beta1 * self.vdb + (1 - beta1) * self.gradient_bias\n",
        "\n",
        "            # Bias correction for adam optimizer for the starting difference while using exponantially weighted average\n",
        "            sdw_corrected = self.sdw / (1 - beta2 ** self.t)\n",
        "            sdb_corrected = self.sdb / (1 - beta2 ** self.t)\n",
        "            vdw_corrected = self.vdw / (1 - beta1 ** self.t)\n",
        "            vdb_corrected = self.vdb / (1 - beta1 ** self.t)\n",
        "\n",
        "            self.sdw_corrected = sdw_corrected\n",
        "            self.sdb_corrected = sdb_corrected\n",
        "            self.vdw_corrected = vdw_corrected\n",
        "            self.vdb_corrected = vdb_corrected\n",
        "\n",
        "    def update(self, learning_rate, optimizer):\n",
        "        if optimizer == 'adam':\n",
        "            self.weight -= learning_rate * self.vdw_corrected / (np.sqrt(self.sdw_corrected) + 1e-8)\n",
        "            self.bias -= learning_rate * self.vdb_corrected / (np.sqrt(self.sdb_corrected) + 1e-8)\n",
        "        else:\n",
        "            self.weight -= learning_rate * self.gradient_weight\n",
        "            self.bias -= learning_rate * self.gradient_bias\n",
        "\n",
        "    def l2(self):\n",
        "        return np.sum(self.weight ** 2)\n",
        "\n",
        "class Relu():\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, gradients):\n",
        "        self.gradient = gradients * (self.input > 0) #why not self.output>>>because we need a boolean return\n",
        "        return self.gradient\n",
        "\n",
        "class Sigmoid():\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "        return self.dinputs\n",
        "\n",
        "class Softmax():\n",
        "    def __init__(self,final=False):\n",
        "        self.final = final\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        probabilities = exp / np.sum(exp, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, gradient):\n",
        "      if self.final == True:\n",
        "        return gradient\n",
        "      else:\n",
        "        self.dinputs = gradient * self.output * (1 - self.output)  # Derivative of softmax\n",
        "        return self.dinputs\n",
        "\n",
        "class CategoricalCrossEntropyLoss():\n",
        "    def forward(self, probs, true_outputs, layers,lamda=0):\n",
        "        clipped_probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
        "        loss_data = -np.sum(true_outputs * np.log(clipped_probs)) / (len(true_outputs) + 1e-8)\n",
        "\n",
        "        l2_terms = [lamda * np.sum(layer.l2()) for layer in layers]\n",
        "        loss_weight = 0.5 * np.sum(l2_terms) / (len(true_outputs) +  1e-8)\n",
        "        return loss_data + loss_weight\n",
        "\n",
        "    def accuracy(self, probs, true_outputs):\n",
        "\n",
        "        prediction=np.argmax(probs, axis=1)\n",
        "        true_label=np.argmax(true_outputs, axis=1)\n",
        "        accuracy=np.mean(prediction == true_label)\n",
        "        return accuracy\n",
        "\n",
        "    def backward(self, probs, true_outputs):\n",
        "        samples = len(true_outputs)\n",
        "\n",
        "        self.dinputs = (probs - true_outputs) / samples\n",
        "        return self.dinputs\n",
        "\n",
        "class BinaryCrossEntropyLoss():\n",
        "    def forward(self, y_pred, y_true, layers):\n",
        "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "        loss_data = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "        return loss_data\n",
        "\n",
        "    def backward(self, dvalues, y_true):\n",
        "        dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "        self.dinputs = (dvalues - y_true) / len(y_true)\n",
        "        return self.dinputs\n"
      ],
      "metadata": {
        "id": "MIEo3IKOzXmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork():\n",
        "  def __init__(self,loss_function='CategoricalCrossEntropyLoss()',optimizer='adam',learning_rate=0.001):\n",
        "     self.layers=[]\n",
        "     self.loss_function = loss_function\n",
        "     self.learning_rate = learning_rate\n",
        "     self.optimizer = optimizer\n",
        "\n",
        "\n",
        "  def add(self,layer):\n",
        "    self.layers.append(layer)\n",
        "\n",
        "\n",
        "\n",
        "  def fit(self, X_train, y_train,X_test,y_test, batch_size,epochs=10):\n",
        "      self.epochs=epochs\n",
        "      for epoch in range(self.epochs):\n",
        "          epoch_loss = 0\n",
        "          epoch_loss_val = 0\n",
        "          for i in range(0, len(X_train), batch_size):\n",
        "              batch_inputs = X_train[i:i + batch_size]\n",
        "              batch_validate=X_test[i:i + batch_size]\n",
        "              batch_true_outputs = y_train[i:i + batch_size]\n",
        "              batch_validate_outputs = y_test[i:i + batch_size]\n",
        "\n",
        "              x = batch_inputs\n",
        "              #print(f'x ko shape{x.shape}')\n",
        "              for layer in self.layers:\n",
        "                  x = layer.forward(x)\n",
        "                  #print(x.shape)\n",
        "\n",
        "\n",
        "              loss = self.loss_function.forward(x, batch_true_outputs, self.layers)\n",
        "              epoch_loss += loss  # Accumulate batch loss\n",
        "\n",
        "              gradient = self.loss_function.backward(x, batch_true_outputs)\n",
        "              for layer in reversed(self.layers):\n",
        "                  # print(f'gradient is {gradient.shape}')\n",
        "                  gradient = layer.backward(gradient)\n",
        "                  # print(f'gradient is {gradient.shape}')\n",
        "\n",
        "              for layer in self.layers:\n",
        "                  layer.calculate(self.optimizer)\n",
        "\n",
        "              for layer in self.layers:\n",
        "                  layer.update(self.learning_rate, self.optimizer)\n",
        "\n",
        "\n",
        "          print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(X_train) * batch_size}\")  # Print average loss for the epoch\n",
        "          epoch_accuracy = 0\n",
        "          epoch_loss_val = 0\n",
        "          # for i in range(0,len(X_test),batch_size):\n",
        "          #     batch_validate = X_test[i:i + batch_size]\n",
        "          #     batch_validate_true = y_test[i:i + batch_size]\n",
        "\n",
        "          x2=X_test\n",
        "          for layer in self.layers:\n",
        "              x2=layer.forward(x2)\n",
        "\n",
        "          loss_validate = self.loss_function.forward(x2, y_test, self.layers)\n",
        "          accurate=self.loss_function.accuracy(x2, y_test)\n",
        "          epoch_loss_val += loss_validate\n",
        "          epoch_accuracy+=accurate\n",
        "          print(f\"Epoch {epoch + 1}, val_Loss: {epoch_loss_val},val_accuracy:{epoch_accuracy}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s0VfoJtp5uh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Load MNIST data\n",
        "data = tf.keras.datasets.mnist\n",
        "(X_train, y_train), (X_test, y_test) = data.load_data()\n",
        "\n",
        "X_train=np.expand_dims(X_train,axis=3)\n",
        "X_train=X_train/255\n",
        "# X_train=X_train.reshape(60000,-1)\n",
        "X_test=np.expand_dims(X_test,axis=3)\n",
        "X_test=X_test/255\n",
        "# X_test=X_test.reshape(10000,-1)\n",
        "# print(X_train[0])\n",
        "y_train = np.eye(10)[y_train]\n",
        "y_test = np.eye(10)[y_test]\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "# y_test=np.random.randn(32,10)\n",
        "# X_train=np.random.randint(0, 256, size=(6000, 28, 28, 1), dtype=np.int64)\n",
        "# X_train=np.float64(X_train)\n",
        "# y_train=np.random.randn(6000,10)\n",
        "# X_test=np.random.randn(32,28,28,1)\n",
        "# #Define network\n",
        "\n",
        "loss_function = CategoricalCrossEntropyLoss()\n",
        "learning_rate = 0.0001\n",
        "optimizer = 'adam'\n",
        "\n",
        "\n",
        "\n",
        "nn=NeuralNetwork(loss_function,optimizer,learning_rate)\n",
        "nn.add(Convolve(Relu(), pooling=MaxPool((2,2)) , filter_size=(3,3),num_filters=8,stride=1,padding=1))\n",
        "nn.add(Convolve(Relu(), pooling=MaxPool((2,2)) , filter_size=(3,3),num_filters=16,stride=1,padding=1))\n",
        "nn.add(Flatten())\n",
        "\n",
        "nn.add(Dense(784,128,Relu()))\n",
        "nn.add(Dense(128,10,Softmax(final=True)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 30\n",
        "\n",
        "epochs = 10\n",
        "nn.fit(X_train, y_train,X_test,y_test, batch_size,30)\n",
        "\n",
        "  # ''', 28, 28, 9\n",
        "  # layer1 = Network(128, 784, lamda=0.01)\n",
        "  # layer2 = Network(64, 128, lamda=0.01)\n",
        "  # layer3 = Network(10, 64, lamda=0)\n",
        "  # relu1 = Relu()\n",
        "\n",
        "  # relu2 = Relu()\n",
        "  # softmax = Softmax()\n",
        "\n",
        "\n",
        "  # layers_for_fit = [layer1, relu1, layer2, relu2, layer3, softmax]\n",
        "  # layers = [layer1, layer2, layer3]\n",
        "  # '''\n",
        "  # # Train model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "GY8c4Wf15aWO",
        "outputId": "451249c6-74f9-4c4e-a0f5-37a39843c9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 10)\n",
            "(10000, 10)\n",
            "Epoch 1, Loss: 2.7270918684342855\n",
            "Epoch 1, val_Loss: 1.2651775577689803,val_accuracy:0.6079\n",
            "Epoch 2, Loss: 1.241023995501714\n",
            "Epoch 2, val_Loss: 1.04431728576858,val_accuracy:0.6423\n",
            "Epoch 3, Loss: 0.9636371621728624\n",
            "Epoch 3, val_Loss: 0.7946877669171629,val_accuracy:0.7304\n",
            "Epoch 4, Loss: 0.8357104757592844\n",
            "Epoch 4, val_Loss: 0.7970775387559589,val_accuracy:0.7295\n",
            "Epoch 5, Loss: 0.754986335644144\n",
            "Epoch 5, val_Loss: 0.6272306992364806,val_accuracy:0.8009\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-50d19a706441>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;31m# ''', 28, 28, 9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-639725c79f48>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_test, y_test, batch_size, epochs)\u001b[0m\n\u001b[1;32m     36\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                   \u001b[0;31m# print(f'gradient is {gradient.shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                   \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                   \u001b[0;31m# print(f'gradient is {gradient.shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-13393d3e439c>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m#print(f'convolve backward{self.gradient.shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;31m#print(f'filter grad{self.filter_grad.shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Reshape the filter gradient to match the original filter dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewaxes_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewshape_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0mbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewaxes_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewshape_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0molda\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moldb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TPl5UkTb__-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eadwopTP7CVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f-vsS2-CVJK0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}